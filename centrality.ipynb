{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "# import my own helper functions\n",
    "from read import read_sims_result\n",
    "from clean import cleanup_0IR_exp\n",
    "from clean import cleanup_network\n",
    "\n",
    "# pearson correlation coeffcient\n",
    "from scipy.stats.stats import pearsonr\n",
    "\n",
    "# deep copy\n",
    "from copy import deepcopy\n",
    "\n",
    "# Page Rank\n",
    "from networkx.algorithms.link_analysis.pagerank_alg import pagerank, pagerank_numpy, pagerank_scipy\n",
    "\n",
    "# my own customized Page Rank\n",
    "from my_page_rank import *\n",
    "\n",
    "# distance\n",
    "from networkx.algorithms.shortest_paths.generic import shortest_path_length\n",
    "\n",
    "# logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# cross validation\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# plot\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# independent variables\n",
    "independent = [\"deposits\", \"cash\", \"assets\", \"credit available\", \"wealth\", \"leverage\", \n",
    "         \"dummy-0-leverage\",\n",
    "         \"wealth-lag\", \"deposits-lag\", \"cash-lag\", \"assets-lag\", \"leverage-lag\", \n",
    "         \"credit-available-lag\", \"credit-issued-lag\", \"dummy-0-leverage-lag\",\n",
    "         \"over-leverage-frequency\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.007, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ###########################\n",
    "# Read OIR results, and fit the model\n",
    "# ###########################\n",
    "df0 = read_sims_result(\"/Users/xcheng/Documents/Oberlin/Summer2/DataAnalysis/data/0622/0IR300s\", 32)\n",
    "df0c = cleanup_0IR_exp(df0, numNode=32, numPeriod=15, numSim=100, balanced=True)\n",
    "\n",
    "X = df0c[independent]\n",
    "y = df0c[\"default-next\"]\n",
    "\n",
    "final = LogisticRegression(penalty=\"l1\", C=0.007)\n",
    "final.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure no defaults in 0IR\n",
    "# sum(df0[df0[\"defaults due to interest\"]\n",
    "#     +df0[\"defaults due to negative wealth\"]\n",
    "#     +df0[\"defaults due to deposit shock\"] == 0].loc[:,\"dot0\":\"dot30\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###########################\n",
    "# Read & process positive IR results\n",
    "# ###########################\n",
    "df_1 = read_sims_result(\"/Users/xcheng/Documents/Oberlin/Summer2/DataAnalysis/data/0625/1IR\", 32)\n",
    "mx_1n = cleanup_network(df_1, numNode=32, numPeriod=15, numSim=50)\n",
    "df_1c = cleanup_0IR_exp(df_1, numNode=32, numPeriod=15, numSim=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_edge_weight(N, dff, mid):\n",
    "    \"\"\"\n",
    "    Calculate weight for edges\n",
    "    Each debt is divided by lenders' wealth (w/o haircut)ï¼Œ\n",
    "    The result number r is scaled to [0, 1) using g(r)=r/(mid+r)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    N: 4D numpy array [n_simulations, n_periods, n_borrowers, n_lenders]\n",
    "        debt adjacency matrices \n",
    "    df: Pandas dataframe (no cleanup)\n",
    "        where we get banks' wealth (w/o haircut)\n",
    "    mid: int\n",
    "        the debt-to-wealth ratio resulting in 50% probability of spreading default\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    WN: 4D numpy array [n_simulations, n_periods, n_borrowers, n_lenders]\n",
    "        new weighted debt adjacency matrices\n",
    "    \"\"\"\n",
    "    \n",
    "    WN = np.copy(N)\n",
    "    simNum, periodNum, bankNum, _= N.shape\n",
    "    dff[\"book_wealth\"] = (dff[\"assets\"] + dff[\"cash\"] + dff[\"debt owed\"] \n",
    "                         - dff[\"debt to pay\"] - dff[\"deposits\"])\n",
    "    \n",
    "    for s in range(simNum):\n",
    "        for p in range(1,periodNum+1):\n",
    "            for lender in range(bankNum):\n",
    "                w = dff[np.array(dff[\"sim#\"]==s) &\n",
    "                        np.array(dff[\"period\"]==p) & \n",
    "                        np.array(dff[\"bankID\"]==lender)\n",
    "                       ][\"book_wealth\"].values[0]\n",
    "                \n",
    "                # helper function\n",
    "                def f(a):\n",
    "                    if a > 0: # there is debt\n",
    "                        if w > 0: # positive wealth\n",
    "                            t = a/w\n",
    "                            return t/(t+mid)\n",
    "                        else: # 0 or negative wealth\n",
    "                            return 100/(100+mid)\n",
    "                    else: # no debt or weird data \n",
    "                        return 0\n",
    "                \n",
    "                WN[s, p-1, :, lender] = [f(k) for k in WN[s, p-1, :, lender]]\n",
    "                    \n",
    "    return WN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_node_weight(N, dff, model, variables):\n",
    "    \"\"\"\n",
    "    Calculate weight for nodes\n",
    "    Each debt is multiplied by lenders' predicted default probability\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    N: 4D numpy array [n_simulations, n_periods, n_borrowers, n_lenders]\n",
    "        debt adjacency matrices \n",
    "    dff: Pandas dataframe (yes cleanup)\n",
    "        where we get bank's balance sheet info\n",
    "    model: model for default probability\n",
    "        scikit learn LogisticRegression\n",
    "    variables: a list of strings\n",
    "        independent variables for the model\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    WN: 3D numpy array [n_simulations, n_periods, n_banks]\n",
    "        array of predicted probability of default\n",
    "    \"\"\"\n",
    "    \n",
    "    simNum, periodNum, bankNum, _= N.shape\n",
    "    WN = np.empty((simNum, periodNum, bankNum))\n",
    "    WN.fill(-1)\n",
    "    \n",
    "    for s in range(simNum):\n",
    "        for p in range(2,periodNum):\n",
    "            for b in range(bankNum):\n",
    "                X = dff[np.array(dff[\"sim#\"]==s) &\n",
    "                        np.array(dff[\"period\"]==p) & \n",
    "                        np.array(dff[\"bankID\"]==b)\n",
    "                       ][variables].values\n",
    "                if X.any():\n",
    "                    predicted_default_probability = model.predict_proba(X)[0][1]\n",
    "                    WN[s, p-1, b] = predicted_default_probability\n",
    "                    \n",
    "    return WN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_weights_all = create_edge_weight(mx_1n, df_1, 0.6)\n",
    "node_weights_all = create_node_weight(mx_1n, df_1c, final, independent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empirically, I discover that it does not matter 0.6 is. I experimented with serveral numbers between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the debt adjacency matrix in a 2d graph\n",
    "# plt.figure(figsize=(10,10))\n",
    "# plt.imshow(edge_weights_all[0,1], interpolation='nearest')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customized_random_walk_exp(N, solvent, random_walk, iterations=10):\n",
    "    \"\"\"\n",
    "    NEW NEW NEW modified random walk algorithm\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    N: 4D numpy array [n_simulations, n_periods, n_borrowers, n_lenders]\n",
    "        debt adjacency matrices \n",
    "    solvent: 3D numpy array [n_simulations, n_periods, n_borrowers, n_lenders] \n",
    "        predicted default probability OR -1\n",
    "    iterations: int\n",
    "        number of iterations\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    result: 1D numpy array\n",
    "        index for the bank's default probability   \n",
    "    \"\"\"\n",
    "    n_s, n_p, n_b, _ = N.shape\n",
    "    s_s, s_p, s_b = solvent.shape\n",
    "    big_result = []\n",
    "    \n",
    "    if n_s != s_s or n_p != s_p or n_b != s_b:\n",
    "        raise ValueError('Two arrays have incompatible sizes.')\n",
    "        \n",
    "    for i, j in np.ndindex((n_s,n_p)):\n",
    "        big_result.extend(random_walk(N[i,j],\n",
    "                                      solvent[i,j],\n",
    "                                      iterations=iterations\n",
    "                                     ).values())\n",
    "        \n",
    "    return np.array(big_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_prediction = customized_random_walk_exp(edge_weights_all, node_weights_all, random_walk_single_1b, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18240"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(overall_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.30186645378375887, 0.0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ###########################\n",
    "# pearson correlation coeffcient is not very big \n",
    "# it gets slightly bigger with lots of iterations\n",
    "# ###########################\n",
    "pearsonr(overall_prediction, df_1c[\"default-next\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sim#</th>\n",
       "      <th>period</th>\n",
       "      <th>bankID</th>\n",
       "      <th>theta (risk aversion)</th>\n",
       "      <th>wealth</th>\n",
       "      <th>deposits</th>\n",
       "      <th>cash</th>\n",
       "      <th>assets</th>\n",
       "      <th>leverage</th>\n",
       "      <th>credit available</th>\n",
       "      <th>...</th>\n",
       "      <th>cash-lag</th>\n",
       "      <th>assets-lag</th>\n",
       "      <th>leverage-lag</th>\n",
       "      <th>credit-available-lag</th>\n",
       "      <th>credit-issued-lag</th>\n",
       "      <th>dummy-0-leverage-lag</th>\n",
       "      <th>over-leverage-frequency</th>\n",
       "      <th>default-next-wealth</th>\n",
       "      <th>default-next-deposit</th>\n",
       "      <th>default-next-interest</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>default-next</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18079</td>\n",
       "      <td>18079</td>\n",
       "      <td>18079</td>\n",
       "      <td>18079</td>\n",
       "      <td>18079</td>\n",
       "      <td>18079</td>\n",
       "      <td>18079</td>\n",
       "      <td>18079</td>\n",
       "      <td>18079</td>\n",
       "      <td>18079</td>\n",
       "      <td>...</td>\n",
       "      <td>18079</td>\n",
       "      <td>18079</td>\n",
       "      <td>18079</td>\n",
       "      <td>18079</td>\n",
       "      <td>18079</td>\n",
       "      <td>18079</td>\n",
       "      <td>18079</td>\n",
       "      <td>18079</td>\n",
       "      <td>18079</td>\n",
       "      <td>18079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>161</td>\n",
       "      <td>161</td>\n",
       "      <td>161</td>\n",
       "      <td>161</td>\n",
       "      <td>161</td>\n",
       "      <td>161</td>\n",
       "      <td>161</td>\n",
       "      <td>161</td>\n",
       "      <td>161</td>\n",
       "      <td>161</td>\n",
       "      <td>...</td>\n",
       "      <td>161</td>\n",
       "      <td>161</td>\n",
       "      <td>161</td>\n",
       "      <td>161</td>\n",
       "      <td>161</td>\n",
       "      <td>161</td>\n",
       "      <td>161</td>\n",
       "      <td>161</td>\n",
       "      <td>161</td>\n",
       "      <td>161</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               sim#  period  bankID  theta (risk aversion)  wealth  deposits  \\\n",
       "default-next                                                                   \n",
       "0             18079   18079   18079                  18079   18079     18079   \n",
       "1               161     161     161                    161     161       161   \n",
       "\n",
       "               cash  assets  leverage  credit available  \\\n",
       "default-next                                              \n",
       "0             18079   18079     18079             18079   \n",
       "1               161     161       161               161   \n",
       "\n",
       "                      ...            cash-lag  assets-lag  leverage-lag  \\\n",
       "default-next          ...                                                 \n",
       "0                     ...               18079       18079         18079   \n",
       "1                     ...                 161         161           161   \n",
       "\n",
       "              credit-available-lag  credit-issued-lag  dummy-0-leverage-lag  \\\n",
       "default-next                                                                  \n",
       "0                            18079              18079                 18079   \n",
       "1                              161                161                   161   \n",
       "\n",
       "              over-leverage-frequency  default-next-wealth  \\\n",
       "default-next                                                 \n",
       "0                               18079                18079   \n",
       "1                                 161                  161   \n",
       "\n",
       "              default-next-deposit  default-next-interest  \n",
       "default-next                                               \n",
       "0                            18079                  18079  \n",
       "1                              161                    161  \n",
       "\n",
       "[2 rows x 24 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1c.groupby(\"default-next\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customized_k_fold(X, y, fold=12):\n",
    "    \"\"\"\n",
    "    customized K-fold cross validation \n",
    "    print certain summary statistics\n",
    "    \"\"\"\n",
    "    # k-fold\n",
    "    kf = KFold(n_splits=fold, shuffle=True)\n",
    "\n",
    "    # initialization\n",
    "    accuracy = 0\n",
    "    conf = np.array([[0, 0], [0, 0]])\n",
    "\n",
    "    def g(A, arr):\n",
    "        if A.shape[1] == 1: return A[arr]\n",
    "        else: return A.loc[arr]\n",
    "    \n",
    "    for train_index, test_index in kf.split(X):\n",
    "#         print(train_index, test_index)\n",
    "        \n",
    "        model = LogisticRegression()\n",
    "        model.fit(g(X,train_index), y.iloc[train_index])\n",
    "        print(model.coef_, model.intercept_)\n",
    "\n",
    "        accuracy += model.score(g(X,test_index), y.iloc[test_index])\n",
    "        conf += confusion_matrix(y.iloc[test_index], model.predict(g(X,test_index)))\n",
    "\n",
    "    tn, fp, fn, tp = conf.ravel()\n",
    "    print(\"{}\\n accuracy:{:24}\\n precision:{:24}\\n recall:{:24}\\n\".format(\n",
    "            conf, \n",
    "            accuracy/fold, \n",
    "            tp/(tp+fp),\n",
    "            tp/(tp+fn)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customized_k_fold_balanced(X, y, fold=12):\n",
    "    \"\"\"\n",
    "    customized K-fold cross validation \n",
    "    print certain summary statistics\n",
    "    \"\"\"\n",
    "    all_obs = np.array(range(len(y)))\n",
    "    default_obs = all_obs[y==1]\n",
    "    nondefault_obs = np.random.choice(all_obs[y==0],\n",
    "                                     size=len(default_obs),\n",
    "                                     replace=False)\n",
    "    all_index = np.append(default_obs, nondefault_obs)\n",
    "\n",
    "    if X.shape[1]== 1:\n",
    "        customized_k_fold(X[all_index], y[all_index], fold=fold)       \n",
    "    else:\n",
    "        customized_k_fold(X.loc[all_index].reset_index(drop=True), \n",
    "                          y[all_index], \n",
    "                          fold=fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01361869]] [-3.90726054]\n",
      "[[0.01397946]] [-3.94548722]\n",
      "[[0.01376178]] [-3.86990437]\n",
      "[[0.0143503]] [-4.03719719]\n",
      "[[0.01384686]] [-3.92518404]\n",
      "[[0.01385552]] [-3.91118102]\n",
      "[[0.01368166]] [-3.90414188]\n",
      "[[0.01443893]] [-3.98620435]\n",
      "[[0.01418632]] [-3.99821625]\n",
      "[[0.01390689]] [-3.98062103]\n",
      "[[0.01370843]] [-3.86930933]\n",
      "[[0.01473468]] [-4.06248995]\n",
      "[[152   9]\n",
      " [  3 158]]\n",
      " accuracy:      0.9627255460588794\n",
      " precision:      0.9461077844311377\n",
      " recall:      0.9813664596273292\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1111111111111111111111111\n",
    "# K-fold cross validation \n",
    "# use approx. default probability (network info) to predict default\n",
    "# ###########################\n",
    "customized_k_fold_balanced(overall_prediction.reshape(-1, 1), df_1c[\"default-next\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# customized_k_fold_balanced(df_1c[\"assets\"].reshape(-1, 1), df_1c[\"default-next\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# customized_k_fold_balanced(node_weights_all.reshape(-1, 1), df_1c[\"default-next\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want train on one balanced dataset, and then test on a different unbalanced dataset.\n",
    "\n",
    "We still the low precision, high recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_balanced(X, y):\n",
    "    \"\"\"\n",
    "    Balanced  the  data, and train a model\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: array-like\n",
    "        training data: independent variables\n",
    "    y: array-like\n",
    "        training data: dependent variables\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    model: Logisitc regression. \n",
    "    \"\"\"\n",
    "    # balance the data\n",
    "    all_obs = np.array(range(len(y)))\n",
    "    default_obs = all_obs[y==1]\n",
    "    nondefault_obs = np.random.choice(all_obs[y==0],\n",
    "                                     size=len(default_obs),\n",
    "                                     replace=False)\n",
    "    all_index = np.append(default_obs, nondefault_obs)\n",
    "    \n",
    "    model = LogisticRegression()\n",
    "    \n",
    "    # whether we have multiple independent variables or just one\n",
    "    # and train  the  model\n",
    "    if X.shape[1]== 1:\n",
    "        model.fit(X[all_index], y[all_index])       \n",
    "    else:\n",
    "        model.fit(X.loc[all_index].reset_index(drop=True), y[all_index])\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01448595]] [-4.17165602]\n"
     ]
    }
   ],
   "source": [
    "# train a logit model on the balanced data\n",
    "# using overall_prediction index to predict default\n",
    "balanced_model = train_balanced(overall_prediction.reshape(-1, 1), df_1c[\"default-next\"])\n",
    "print(balanced_model.coef_, balanced_model.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[33191  2885]\n",
      " [   20   310]]\n",
      " accuracy:      0.9202054606383563\n",
      " precision:     0.09702660406885759\n",
      " recall:      0.9393939393939394\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prepare a second dataset, validate\n",
    "df_100 = read_sims_result(\"/Users/xcheng/Documents/Oberlin/Summer2/DataAnalysis/data/0719/1IR100s\", 32)\n",
    "mx_100n = cleanup_network(df_100, numNode=32, numPeriod=15, numSim=100)\n",
    "df_100c = cleanup_0IR_exp(df_100, numNode=32, numPeriod=15, numSim=100)\n",
    "\n",
    "edge_weights_100 = create_edge_weight(mx_100n, df_100, 0.6)\n",
    "node_weights_100 = create_node_weight(mx_100n, df_100c, final, independent)\n",
    "\n",
    "overall_pred_100 = customized_random_walk_exp(edge_weights_100, node_weights_100, random_walk_single_1b, 500)\n",
    "\n",
    "accuracy = balanced_model.score(overall_pred_100.reshape(-1, 1), df_100c[\"default-next\"])\n",
    "conf = confusion_matrix(df_100c[\"default-next\"], balanced_model.predict(overall_pred_100.reshape(-1, 1)))\n",
    "tn, fp, fn, tp = conf.ravel()\n",
    "print(\"{}\\n accuracy:{:24}\\n precision:{:24}\\n recall:{:24}\\n\".format(\n",
    "        conf, \n",
    "        accuracy, \n",
    "        tp/(tp+fp),\n",
    "        tp/(tp+fn)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction based on logistic regression with balanced sheet information\n",
    "bs_prediction_all = node_weights_all.reshape(1,-1)[0]\n",
    "bal_sh_prediction = bs_prediction_all [bs_prediction_all != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip two predictions into a dataframe\n",
    "two_predictions = pd.DataFrame({'balanced sheet':bal_sh_prediction, 'network':overall_prediction})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.82227979 0.01355207]] [-4.20212574]\n",
      "[[1.32219723 0.01149829]] [-3.95625961]\n",
      "[[1.39640341 0.01128357]] [-3.95008216]\n",
      "[[0.93016178 0.01308366]] [-4.172605]\n",
      "[[1.25382444 0.01195987]] [-4.0325485]\n",
      "[[1.31984862 0.01175224]] [-3.9977899]\n",
      "[[1.37974704 0.01172133]] [-4.00892144]\n",
      "[[1.21228164 0.01169776]] [-3.94069053]\n",
      "[[1.17756964 0.01191752]] [-3.98700097]\n",
      "[[1.12306241 0.01205025]] [-3.90697603]\n",
      "[[1.21085247 0.01178642]] [-4.02767211]\n",
      "[[1.36758064 0.01173838]] [-4.02730538]\n",
      "[[148  13]\n",
      " [  5 156]]\n",
      " accuracy:       0.944207027540361\n",
      " precision:      0.9230769230769231\n",
      " recall:       0.968944099378882\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 222222222222222222222222222\n",
    "# K-fold cross validation\n",
    "# use approx. default probability + balanced sheet default prob to predict default\n",
    "# This does not seem to be an effective approach\n",
    "# ###########################\n",
    "customized_k_fold_balanced(two_predictions, df_1c[\"default-next\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using network information (4 modified page rank I created) as the sole predictor seems to predict the result fairly well. It is significantly better than using a single balanced sheet property or balanced sheet default prediction (logisitis regression) to do the same thing.\n",
    "\n",
    "Combining (logsitic regression) balanced sheet default prediction (logisitis regression) with network information (4 modified page rank I created) does not improve results significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------\n",
    "Stuff Below this are old stuff that I might or might not need.\n",
    "----------------------------------------------------------------------\n",
    "----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_avg_max(N):\n",
    "    \"\"\"\n",
    "    calculate average & max distances between all pair of nodes\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    N: 4D numpy array [n_simulations, n_periods, n_borrowers, n_lenders]\n",
    "        debt adjacency matrices \n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    avg_d: 2D numpy array [n_simulations, n_periods]\n",
    "        average distances between all pair of nodes\n",
    "    max_d: 2D numpy array [n_simulations, n_periods]\n",
    "        max distances between all pair of nodes\n",
    "    \"\"\"\n",
    "    numSim, numPeriod, _, _ = N.shape\n",
    "    avg_d = np.empty((numSim, numPeriod-2))\n",
    "    max_d = np.empty((numSim, numPeriod-2))\n",
    "    \n",
    "    for s in range(numSim):\n",
    "        for p in range(1,numPeriod-1):\n",
    "            disG = nx.DiGraph(N[s,p])\n",
    "            dists = shortest_path_length(disG, weight=None)\n",
    "            curlist=[]\n",
    "            for source in dists:\n",
    "                curlist.extend(source[1].values())\n",
    "            avg_d[s,p-1] = sum(curlist) / float(len(curlist))\n",
    "            max_d[s,p-1] = max(curlist)\n",
    "            \n",
    "    return avg_d, max_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###########################\n",
    "# Visualize max/avg distances between banks\n",
    "# ###########################\n",
    "# avgg, maxx = dist_avg_max(mx_1n)\n",
    "#\n",
    "# pavg = pd.DataFrame(avgg)\n",
    "# pmax = pd.DataFrame(maxx)\n",
    "# # pavg.mean().plot()\n",
    "# abc = pmax.stack().value_counts().sort_index().plot(\n",
    "#     kind=\"bar\",\n",
    "#     title=\"max distances, 1 interest rates, 50 simulations, 15 periods\",\n",
    "#     figsize=(8,6),\n",
    "#     fontsize=12\n",
    "# )\n",
    "# abc.set_xlabel(\"max distance between any pair of reachable nodes\")\n",
    "# abc.set_ylabel(\"frequncy\")\n",
    "# abc.title.set_fontsize(15)\n",
    "# abc.xaxis.label.set_fontsize(15)\n",
    "# abc.yaxis.label.set_fontsize(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weigh_networks(N, model, variables):\n",
    "    \"\"\"\n",
    "    Add weight to network\n",
    "    Each debt is multiplied by lenders' predicted default probability\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    N: 4D numpy array [n_simulations, n_periods, n_borrowers, n_lenders]\n",
    "        debt adjacency matrices \n",
    "    model: scikit learn LogisticRegression\n",
    "        model for default probability\n",
    "    variables: a list of strings\n",
    "        independent variables for the model\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    WN: 4D numpy array [n_simulations, n_periods, n_borrowers, n_lenders]\n",
    "        new weighted debt adjacency matrices\n",
    "    \"\"\"\n",
    "    \n",
    "    WN = np.copy(N)\n",
    "    simNum, periodNum, bankNum, _= N.shape\n",
    "    \n",
    "    for s in range(simNum):\n",
    "        for p in range(1,periodNum-1):\n",
    "            for b in range(bankNum):\n",
    "                X = df_1c[np.array(df_1c[\"sim#\"]==s) &\n",
    "                          np.array(df_1c[\"period\"]==p) & \n",
    "                          np.array(df_1c[\"bankID\"]==b)\n",
    "                         ][variables].values\n",
    "                if X.any():\n",
    "                    predicted_default_probability = model.predict_proba(X)[0][1]\n",
    "                    WN[s, p-1, b] *= predicted_default_probability\n",
    "                    \n",
    "    return WN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_pagerank_numpy(G, alpha=0.85, personalization=None, weight='weight', dangling=None):\n",
    "    \"\"\"\n",
    "    This is basically pagerank_numpy without normalization.\n",
    "    \"\"\"\n",
    "    from networkx.algorithms.link_analysis.pagerank_alg import google_matrix\n",
    "    \n",
    "    if len(G) == 0:\n",
    "        return {}\n",
    "    M = google_matrix(G, alpha, personalization=personalization,\n",
    "                      weight=weight, dangling=dangling)\n",
    "    # use numpy LAPACK solver\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(M.T)\n",
    "    ind = np.argmax(eigenvalues)\n",
    "    # eigenvector of largest eigenvalue is at ind\n",
    "    largest = np.array(eigenvectors[:, ind]).flatten().real\n",
    "    return dict(zip(G, map(float, abs(largest))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_to_networks(f, N):\n",
    "    \"\"\"\n",
    "    Calculate Page Rank scores for all the networks \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    f: function (2D numpy array -> matrix)\n",
    "        the function to apply to each network (e.g. Page Rank)\n",
    "    N: 4D numpy array [n_simulations, n_periods, n_borrowers, n_lenders]\n",
    "        debt adjacency matrices (netowrks)\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    PG: 3D numpy array [n_simulations, n_periods, n_banks]\n",
    "        Page Rank scores\n",
    "    \"\"\"\n",
    "    \n",
    "    simNum, periodNum, bankNum, _= N.shape\n",
    "    PG = np.empty((simNum, periodNum, bankNum))\n",
    "    \n",
    "    for s in range(simNum):\n",
    "        for p in range(1,periodNum-1):\n",
    "            PG[s, p] = np.array(list(f(nx.DiGraph(N[s, p])).values()))\n",
    "            \n",
    "    return PG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###########################\n",
    "# Let's add the weight\n",
    "# ###########################\n",
    "weighted = weigh_networks(mx_1n, final, independent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###########################\n",
    "# Let's calculate pagerank\n",
    "# ###########################\n",
    "pg_iter = apply_to_networks(pagerank, weighted)\n",
    "pg_norm = apply_to_networks(pagerank_numpy, weighted)\n",
    "pg_not_norm = apply_to_networks(my_pagerank_numpy, weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
