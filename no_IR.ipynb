{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ####################################\n",
    "# Settings & Imports\n",
    "# ####################################\n",
    "\n",
    "# Imports from __future__ in case we're running Python 2\n",
    "from __future__ import division, print_function\n",
    "from __future__ import absolute_import, unicode_literals\n",
    "\n",
    "# import my own helper functions\n",
    "from read import read_sims_result\n",
    "from clean import cleanup_0IR_exp\n",
    "from clean import cleanup_0IR_single\n",
    "\n",
    "# Our numerical workhorses\n",
    "import numpy as np\n",
    "\n",
    "# Import pyplot for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import logistic regression from scikit learn \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# import model selection stuff from scikit learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# import feature selection stuff from scikit learn\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# sklearn.metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import brier_score_loss\n",
    "\n",
    "#\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# \n",
    "# from sklearn import preprocessing\n",
    "\n",
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Seaborn, useful for graphics\n",
    "import seaborn as sns\n",
    "\n",
    "# Magic function to make matplotlib inline; other style specs must come AFTER\n",
    "%matplotlib inline\n",
    "\n",
    "# This enables SVG graphics inline.  There is a bug, so uncomment if it works.\n",
    "# %config InlineBackend.figure_formats = {'svg',}\n",
    "\n",
    "# This enables high resolution PNGs. SVG is preferred, but has problems\n",
    "# rendering vertical and horizontal lines\n",
    "# %config InlineBackend.figure_formats = {'png', 'retina'}\n",
    "\n",
    "# JB's favorite Seaborn settings for notebooks\n",
    "rc = {'lines.linewidth': 2, \n",
    "      'axes.labelsize': 18, \n",
    "      'axes.titlesize': 18, \n",
    "      'axes.facecolor': 'DFDFE5'}\n",
    "# sns.set_context('notebook', rc=rc)\n",
    "# sns.set_style('darkgrid', rc=rc)\n",
    "\n",
    "# remove some pandas warning\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to construct a model to estimate the probability of default based on some parameter when this is no borrowing or lending.\n",
    "\n",
    "First, let's read and clean up the data.\n",
    "\n",
    "Then, we observe the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ####################################\n",
    "# Actually run analysis, toggle env variable here\n",
    "# ####################################\n",
    "df = read_sims_result(\"/Users/xcheng/Documents/Oberlin/Summer2/DataAnalysis/data/0622/0IR\", 32)\n",
    "df2 = cleanup_0IR_exp(df, numNode=32, numPeriod=15, numSim=200, balanced=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ####################################\n",
    "# Make Sure There is NO DEBT in this 0IR simulation\n",
    "# ####################################\n",
    "# df3 = df[df[\"defaults due to negative wealth\"] # filter default rows\n",
    "#   +df[\"defaults due to deposit shock\"]\n",
    "#   +df['defaults due to interest'] == 0].copy()\n",
    "# for col in df.columns.values[:31]:\n",
    "#     print('{0:5s} {1:15f} {2:15f}'.format(col, df3[col].sum(), df[col].sum()))\n",
    "# print(\"total debt to pay:\", df3[\"debt to pay\"].sum())\n",
    "# print(\"total debt owed:\", df3[\"debt owed\"].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ####################################\n",
    "# look at the data\n",
    "# ####################################\n",
    "# k=1\n",
    "# sim1 = df.loc[k*15*31:(k+1)*15*31-1]\n",
    "# sim1_ready = cleanup_0IR_single(sim1, 32)\n",
    "# sim1_ready[sim1_ready[\"theta (risk aversion)\"] == 0.05]\n",
    "# sim2 = df.loc[49*15*31:50*15*31-1].copy().reset_index(drop=True)\n",
    "# sim2_ready = cleanup_0IR_single(sim2, 32)\n",
    "# sim2_ready\n",
    "# sim2_ready[sim2_ready[\"theta (risk aversion)\"] == 0.05].loc[:,\"period\":]\n",
    "# df2[df2[\"theta (risk aversion)\"] == 0.05].loc[:,\"period\":]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ####################################\n",
    "# look at how things differ for banks with different level of risk aversion\n",
    "# ####################################\n",
    "# avg_by_bank = df2.groupby('theta (risk aversion)').mean()\n",
    "# avg_by_bank[[\"default-next-wealth\", \"default-next-deposit\"]].plot(\n",
    "#     kind=\"line\", \n",
    "#     figsize=(12,8),\n",
    "#     title=\"Frequency & Reason of Default for Each Bank\"\n",
    "# )\n",
    "# avg_by_bank[[\"wealth\", \"deposits\", \"cash\", \"assets\"]].plot(\n",
    "#     kind=\"line\", \n",
    "#     figsize=(12,8),\n",
    "#     title=\"Investment Decisions for Each Bank\"\n",
    "# )\n",
    "# avg_by_bank[[\"credit available\", \"credit issued\"]].plot(\n",
    "#     kind=\"line\", \n",
    "#     figsize=(12,8),\n",
    "#     title=\"Credit Condition for Each Bank\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ####################################\n",
    "# look at the plots for correlation\n",
    "# ####################################\n",
    "\n",
    "# to see how independent variables correlate with each other \n",
    "# plt.scatter(df2[\"cash\"], df2[\"credit available\"])\n",
    "# plt.scatter(df2[\"assets\"], df2[\"credit available\"])\n",
    "# plt.scatter(df2[\"deposits\"], df2[\"credit available\"])\n",
    "# plt.scatter(df2[\"cash\"], df2[\"assets\"])\n",
    "# plt.scatter (df2[\"cash\"], df2[\"deposits\"])\n",
    "# plt.scatter(df2[\"deposits\"], df2[\"assets\"])\n",
    "\n",
    "# to see how dependent variable relates to independent variables\n",
    "# plt.scatter(df2[\"default-next\"], df2[\"cash\"])\n",
    "# plt.scatter(df2[\"default-next\"], df2[\"assets\"])\n",
    "# plt.scatter(df2[\"default-next\"], df2[\"deposits\"])\n",
    "# plt.scatter(df2[\"default-next\"], df2[\"wealth\"])\n",
    "\n",
    "# plot settings\n",
    "# plt.yscale(\"symlog\")\n",
    "# plt.xscale(\"symlog\")\n",
    "# plt.rcParams[\"figure.figsize\"] = [25,15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ####################################\n",
    "# look at how things differ during each period\n",
    "# ####################################\n",
    "\n",
    "# Plot time vs default\n",
    "# df2['period'].hist(by=df2['default-next'], bins=14,  rwidth=0.7, stacked=True)\n",
    "\n",
    "# Plot time vs independent variables\n",
    "# avg_by_time = df2.groupby('period').mean()\n",
    "# avg_by_time[[\"default-next-wealth\", \"default-next-deposit\"]].plot(\n",
    "#     kind=\"line\", \n",
    "#     figsize=(12,8),\n",
    "#     title=\"Frequency & Reason of Default During Each Period\"\n",
    "# )\n",
    "# avg_by_time[[\"wealth\", \"deposits\", \"cash\", \"assets\"]].plot(\n",
    "#     kind=\"line\", \n",
    "#     figsize=(12,8),\n",
    "#     title=\"Investment Decisions During Each Period\"\n",
    "# )\n",
    "# avg_by_time[[\"credit available\", \"credit issued\"]].plot(\n",
    "#     kind=\"line\", \n",
    "#     figsize=(12,8),\n",
    "#     title=\"Credit Condition During Each Period\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative wealth not default\n",
    "# df[df[\"wealth\"]<0].loc[1:10000,\"period\":]\n",
    "# df2[df2[\"wealth\"]<=0][\"wealth\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# come back from 0 wealth\n",
    "# df2[df2[\"theta (risk aversion)\"]==0.1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The thing about negative wealth is due to the haircut in asset when calculating wealth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the number of default vs non-default cases\n",
    "# df2.groupby('default-next').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to figure out whether there is Multicollinearity\n",
    "# df2[\"cash\"]+df2[\"assets\"]*0.8-df2[\"deposits\"]-df2[\"wealth\"]\n",
    "# np.linalg.eig(X.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to construst a model to predict default (in the next period) in a simulation where there is no lending or borrowing. \n",
    "\n",
    "Variables we can consider includes deposits, cash, assets, credit available, credit issued. \n",
    "\n",
    "I exclude theta (risk aversion) because I think it is private info. \n",
    "\n",
    "I exclude period because I don't think banks should have this information. It feels like cheating.\n",
    "\n",
    "I exclude defaults due to interest, debt to pay, debt owed, over leverages because there is no debt. \n",
    "\n",
    "I exclude credit issued becuase this should not affect anything when there is no debt. \n",
    "\n",
    "I exclude wealth because wealth is a linear combination of deposits, cash and assets.\n",
    "(this might not be true)\n",
    "\n",
    "Let's do a logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# independent variables (candidates)\n",
    "independent = [\"deposits\", \"cash\", \"assets\", \"credit available\", \"wealth\", \"leverage\", \n",
    "         \"dummy-0-leverage\",\n",
    "         \"wealth-lag\", \"deposits-lag\", \"cash-lag\", \"assets-lag\", \"leverage-lag\", \n",
    "         \"credit-available-lag\", \"credit-issued-lag\", \"dummy-0-leverage-lag\",\n",
    "         \"over-leverage-frequency\"]\n",
    "# dependent variable 0\n",
    "dependent = \"default-next\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df2[independent]\n",
    "y = df2[dependent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our data, there are significantly more non-default cases than default cases after I remove cases where the banks default during the previous periods. Over 98% of the cases are non-default. So when we do cross validation, the model we train would just predict not default every time and still get a very high accuracy. \n",
    "\n",
    "[DONE] Solution : Throw away lots of non-default cases randomly\n",
    "    -> get the same amount of default and non-default cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation matrix\n",
    "# X.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We sometimes get a warning about complete quasi-separation. What does that mean? Trying to figure out.\n",
    "\n",
    "Once I remove cash as an independent variable, the warning about possible complete quasi-separation goes away. However, I don't think we should readlly care about complete quasi-separation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RFE feature selection\n",
    "# logreg = LogisticRegression()\n",
    "# logreg.fit(X,y)\n",
    "# chosen_rfe_X = []\n",
    "# for i in range(1,6):\n",
    "#     rfe = RFE(logreg, i)\n",
    "#     rfe = rfe.fit(X,y)\n",
    "#     print(i, \"feature(s):\", rfe.support_, rfe.ranking_)\n",
    "#     chosen_rfe_X.append(X.columns.values[rfe.support_])\n",
    "# chosen_rfe_X.append(X.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model with features selected by RFE\n",
    "# for predictors in chosen_rfe_X:\n",
    "#     print(sm.Logit(y, sm.add_constant(df2[predictors])).fit().summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the prediction\n",
    "# m = LogisticRegression()\n",
    "# m.fit(df2[[\"cash\", \"assets\"]], df2[\"default-next\"])\n",
    "# print(m.intercept_, m.coef_)\n",
    "# print(m.predict([[0,50], [50,0],[50,50],[-100,-100],[1000,1000]]))\n",
    "# print(m.predict_proba([[10,0.9]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##############################\n",
    "# K-fold cross validation\n",
    "# ##############################\n",
    "# fold = 9\n",
    "# kf = KFold(n_splits=fold, shuffle=True)\n",
    "\n",
    "# for pp in chosen_rfe_X[1:]:\n",
    "#     xx = df2[pp]\n",
    "#     score = 0\n",
    "# #     f1 = 0\n",
    "#     con = np.array([[0, 0], [0, 0]])\n",
    "# #     mat = 0\n",
    "#     brier = 0\n",
    "#     for train_index, test_index in kf.split(xx):\n",
    "#         logreg = LogisticRegression()\n",
    "#         logreg.fit(xx.iloc[train_index], y.iloc[train_index])\n",
    "#         score += logreg.score(xx.iloc[test_index], y.iloc[test_index])\n",
    "#         con += confusion_matrix(y.iloc[test_index], logreg.predict(xx.iloc[test_index]))\n",
    "# #         f1 += f1_score(y.iloc[test_index], logreg.predict(xx.iloc[test_index]))\n",
    "# #         mat += matthews_corrcoef(y.iloc[test_index], logreg.predict(xx.iloc[test_index]))\n",
    "#         brier += brier_score_loss(y.iloc[test_index], logreg.predict(xx.iloc[test_index]))\n",
    "#     print(\"{}\\n {}\\n accuracy:{:24}\\n  brier:{:24}\\n\".format(\n",
    "#         xx.columns.values, con, score/fold, brier/fold))\n",
    "    \n",
    "\n",
    "# for p in X.columns.values:\n",
    "#     xx = df2[p].values.reshape(-1, 1)\n",
    "#     score = 0\n",
    "# #     f1 = 0\n",
    "#     con = np.array([[0, 0], [0, 0]])\n",
    "#     brier = 0\n",
    "#     for train_index, test_index in kf.split(xx):\n",
    "#         logreg = LogisticRegression()\n",
    "#         logreg.fit(xx[train_index], y[train_index])\n",
    "#         score += logreg.score(xx[test_index], y[test_index])\n",
    "#         con += confusion_matrix(y[test_index], logreg.predict(xx[test_index]))\n",
    "# #         f1 += f1_score(y[test_index], logreg.predict(xx[test_index]))\n",
    "#         brier += brier_score_loss(y[test_index], logreg.predict(xx[test_index]))\n",
    "#     print(\"{}\\n {}\\n accuracy:{:24}\\n brier:{:24}\\n\".format(\n",
    "#         p, con, score/fold, brier/fold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##############################\n",
    "# Regularization + cross validation \n",
    "# trial and error for the best lambda\n",
    "# ##############################\n",
    "# fold = 12\n",
    "# kf = KFold(n_splits=fold, shuffle=True)\n",
    "\n",
    "# accuracy = 0\n",
    "# conf = np.array([[0, 0], [0, 0]])\n",
    "# brier = 0\n",
    "# co_effs = pd.DataFrame(columns = np.append(X.columns.values, \"const\"), \n",
    "#                        index=range(fold))\n",
    "# row = 0\n",
    "\n",
    "# for train_index, test_index in kf.split(X):\n",
    "#     lasso = LogisticRegression(penalty=\"l1\", C=0.007)\n",
    "#     lasso.fit(X.iloc[train_index], y.iloc[train_index])\n",
    "#     new_row = np.append(lasso.coef_, lasso.intercept_)\n",
    "#     co_effs.loc[row] = new_row\n",
    "#     accuracy += lasso.score(X.iloc[test_index], y.iloc[test_index])\n",
    "#     conf += confusion_matrix(y.iloc[test_index], lasso.predict(X.iloc[test_index]))\n",
    "#     brier += brier_score_loss(y.iloc[test_index], lasso.predict(X.iloc[test_index]))\n",
    "#     row += 1\n",
    "# print(\"{}\\n accuracy:{:24}\\n brier:{:24}\\n\".format(\n",
    "#         conf, accuracy/fold, brier/fold))\n",
    "# co_effs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.007, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final = LogisticRegression(penalty=\"l1\", C=0.007)\n",
    "final.fit(X,y)\n",
    "print(final.coef_, final.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##############################\n",
    "# Read input for 1 interest rate\n",
    "# ##############################\n",
    "df_1 =  read_sims_result(\"/Users/xcheng/Documents/Oberlin/Summer2/DataAnalysis/data/0614/1IR\", 32)\n",
    "df_1c = cleanup_0IR_exp(df_1, numNode=32, numPeriod=15, numSim=50, balanced=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9093400687876182\n",
      "[[16786  1686]\n",
      " [    1   135]]\n",
      "0.09065993121238178\n"
     ]
    }
   ],
   "source": [
    "# ##############################\n",
    "# Examine the prediction interest rate\n",
    "# ##############################\n",
    "print(final.score(df_1c[independent], df_1c[dependent]))\n",
    "print(confusion_matrix(df_1c[dependent], final.predict(df_1c[independent])))\n",
    "print(brier_score_loss(df_1c[dependent], final.predict(df_1c[independent])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##############################\n",
    "# Read input for 2 interest rates\n",
    "# ##############################\n",
    "df_2 =  read_sims_result(\"/Users/xcheng/Documents/Oberlin/Summer2/DataAnalysis/data/0614/2IR\", 32)\n",
    "df_2c = cleanup_0IR_exp(df_2, numNode=32, numPeriod=15, numSim=50, balanced=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8994513553153349\n",
      "[[16407  1848]\n",
      " [    3   151]]\n",
      "0.10054864468466511\n"
     ]
    }
   ],
   "source": [
    "# ##############################\n",
    "# Examine the prediction for 2 interest rates\n",
    "# ##############################\n",
    "print(final.score(df_2c[independent], df_2c[dependent]))\n",
    "print(confusion_matrix(df_2c[dependent], final.predict(df_2c[independent])))\n",
    "print(brier_score_loss(df_2c[dependent], final.predict(df_2c[independent])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the prediction is fairly good."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
